[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Surya Vardhan",
    "section": "",
    "text": "Welcome to my corner of the internet! I’m Surya Vardhan, a passionate individual deeply engaged in the world of technology and data. With a curiosity-driven mindset, I explore the realms of data science, machine learning, and beyond. Join me on this journey of discovery and innovation!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "The provided R code loads the mtcars data set and conducts a linear regression analysis to predict miles per gallon (mpg) based on the independent variables of weight (wt), horsepower (hp), and quarter-mile time (qsec). The resulting regression model is stored in the variable model. The summary of the model, generated using the summary function, offers detailed information on coefficients, standard errors, t-values, and p-values, providing insights into the relationships between the predictor variables and the target variable. This summary aids in interpreting the significance of each predictor in explaining the variation in miles per gallon in the context of the dataset.\n\n\nCode\n# Load the mtcars dataset (if not already loaded)\ndata(mtcars)\n\n# Perform linear regression (e.g., predicting mpg based on other variables)\nmodel &lt;- lm(mpg ~ wt + hp + qsec, data = mtcars)\n\n# Display the summary of the regression model\nsummary(model)\n\n\n\nCall:\nlm(formula = mpg ~ wt + hp + qsec, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8591 -1.6418 -0.4636  1.1940  5.6092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 27.61053    8.41993   3.279  0.00278 ** \nwt          -4.35880    0.75270  -5.791 3.22e-06 ***\nhp          -0.01782    0.01498  -1.190  0.24418    \nqsec         0.51083    0.43922   1.163  0.25463    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.578 on 28 degrees of freedom\nMultiple R-squared:  0.8348,    Adjusted R-squared:  0.8171 \nF-statistic: 47.15 on 3 and 28 DF,  p-value: 4.506e-11\n\n\nThe linear regression analysis reveals a significant relationship between the predictor variables (weight, horsepower, and quarter-mile time) and the target variable (miles per gallon). The model suggests that weight has a substantial negative impact on mpg, while there is no statistically significant effect from horsepower and quarter-mile time. The overall model is highly significant (p-value: 4.506e-11) with a strong explanatory power (Adjusted R-squared: 0.8171), indicating that the chosen variables collectively account for a significant portion of the variance in miles per gallon in the mtcars dataset."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my blog. Welcome!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Surya Vardhan",
    "section": "",
    "text": "Welcome to my corner of the internet! I’m Surya Vardhan, a passionate individual deeply engaged in the world of technology and data. With a curiosity-driven mindset, I explore the realms of data science, machine learning, and beyond. Join me on this journey of discovery and innovation!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Introduction to Linear Regression using SAS Software\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSurya Vadham\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Linear Regression Analysis\n\n\n\nnews\n\n\nanalysis\n\n\nplotly\n\n\nplot\n\n\n\n\n\n\n\nSurya Vadham\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nSurya Vardham\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post with plot/index.html",
    "href": "posts/post with plot/index.html",
    "title": "Understanding Linear Regression Analysis",
    "section": "",
    "text": "Linear Regression Analysis is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The primary goal is to establish a linear equation that best describes the association between these variables. This analysis is widely employed in various fields, including economics, finance, biology, and social sciences, to make predictions and understand the underlying patterns in the data."
  },
  {
    "objectID": "posts/post with code/index.html",
    "href": "posts/post with code/index.html",
    "title": "Introduction to Linear Regression using SAS Software",
    "section": "",
    "text": "Linear regression is a statistical method widely employed for modeling the relationship between a dependent variable and one or more independent variables. SAS (Statistical Analysis System) software offers a powerful platform for performing linear regression analysis, providing users with robust tools to explore, analyze, and interpret relationships within their datasets.\nSAS’s linear regression capabilities are accessible through procedures such as PROC REG, which stands for regression analysis. This procedure allows users to fit linear models to their data, enabling the estimation of parameters, assessment of model fit, and examination of variable relationships.\nTo initiate a linear regression analysis in SAS, users typically start by specifying the PROC REG statement, followed by the variable names corresponding to the dependent and independent variables. For example: PROC REG DATA=mydata; MODEL dependent_variable = independent_variable1 independent_variable2; RUN; The output from PROC REG includes various tables that provide valuable information for interpreting the linear regression model. The “Model Fit” table, for instance, presents crucial statistics such as the R-squared, which measures the proportion of variability in the dependent variable explained by the model. A higher R-squared indicates a better fit.\nCoefficients for each independent variable are reported in the “Parameter Estimates” table, showing the estimated slope (beta) and intercept (alpha) values. These coefficients help quantify the strength and direction of the relationships between variables.\nDiagnostic statistics, like standard errors, t-values, and p-values, aid in assessing the significance of the coefficients. A low p-value indicates that the corresponding variable significantly contributes to the model. SAS software also facilitates the visualization of linear regression results through graphical tools. The “Plot” statement within PROC REG generates scatterplots with regression lines, aiding in the visual interpretation of the relationships between variables.\nFurthermore, SAS allows users to perform various regression diagnostics, such as checking for multicollinearity, influential observations, and normality of residuals. These diagnostics help ensure the validity of the linear regression model and identify potential issues that may affect its reliability."
  },
  {
    "objectID": "posts/final project/index.html",
    "href": "posts/final project/index.html",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "In this group project, you will work with analysts’ forecast data of earning per share (EPS) provided by Wharton Research Data Services (WRDS). Institutional Brokers’ Estimate System (I/B/E/S) provides historical data on certain financial indicators collected from thousands of individual analysts working in more than 3,000 broker houses.\n\nTICKER: A unique identifier assigned to each security. In this group project, you will only model “NFLX” ticker.\nCNAME: Company name\nACTDATS: The Activation date: It is the date when the analyst forecast became effective within the IBES database.\nESTIMATOR: Sellside institution (mostly broker house). It is just the broker.\nANALYS: The person who makes the forecast and work for sellside institution. Estimators and analysts are represented by codes to hide their real names.\nFPI: Forecast Period Indicator: The forecasting period. 6: Next Fiscal Quarter 1: Next Fiscal Year\nMEASURE: The variable being estimated. We have data for earning per share (EPS)\nVALUE: The forecasted value of EPS\nFPEDATS: The Forecast Period End Date: It is the ending date of the fiscal period to which the estimate applies. For the majority of companies, the FPEDATS date is December 31st of that year.\nREVDATS: The Review Date: It is the most recent date on which IBES called the analyst and verified that particular estimate as still valid for that analyst. If an analyst confirms that a previous estimate is still valid, the original database record for that estimate is retained and only the REVDATS variable is updated. If an analyst changes their estimate for a given company, a new record is entered in the database with a new ANNDATS. The old record of the analyst (containing the previous estimate) is retained in the database.\nREVTIMS: Time-stamp of REVDATS\nANNDATS: The Announce date: It is the date on which the analyst first made that particular estimate.\nANNTIMS: Time-stamp of ANNDATS\nACTUAL: The realized EPS, the true EPS value.\nANNDATS_ACT: The Announced date of Actual EPS: The actual EPS value is announced by the company at this date.\nANNTIMS_ACT: The time-stamp of ANNDATS_ACT\n\n\n\nCode\n# Load required libraries\nif (!requireNamespace(\"data.table\", quietly = TRUE)) {\n  install.packages(\"data.table\")\n}\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(corrplot)\n\n\ncorrplot 0.92 loaded\n\n\nCode\nlibrary(data.table)\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nCode\nlibrary(kableExtra)\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nCode\nlibrary(reshape2)\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following objects are masked from 'package:data.table':\n\n    dcast, melt\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nCode\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nCode\n# Read CSV file\nNFLX &lt;- read.csv(\"C:/Users/HP/Downloads/NFLX.csv\", header=TRUE)\n\n# Display column names\nnames(NFLX)\n\n\n [1] \"TICKER\"      \"CNAME\"       \"ACTDATS\"     \"ESTIMATOR\"   \"ANALYS\"     \n [6] \"FPI\"         \"MEASURE\"     \"VALUE\"       \"FPEDATS\"     \"REVDATS\"    \n[11] \"REVTIMS\"     \"ANNDATS\"     \"ANNTIMS\"     \"ACTUAL\"      \"ANNDATS_ACT\"\n[16] \"ANNTIMS_ACT\"\n\n\nCode\n# View first rows\nhead(NFLX)\n\n\n  TICKER        CNAME  ACTDATS ESTIMATOR ANALYS FPI MEASURE   VALUE  FPEDATS\n1   NFLX NETFLIX INC. 20020812      1872   6749   6     EPS -0.0086 20020930\n2   NFLX NETFLIX INC. 20020812       183  79868   6     EPS -0.0029 20020930\n3   NFLX NETFLIX INC. 20020812       220  57596   6     EPS -0.0100 20020930\n4   NFLX NETFLIX INC. 20021009       183  79868   6     EPS -0.0050 20020930\n5   NFLX NETFLIX INC. 20020805       183  79868   1     EPS -0.0250 20021231\n6   NFLX NETFLIX INC. 20021202      2178  80485   1     EPS -0.0321 20021231\n   REVDATS  REVTIMS  ANNDATS  ANNTIMS ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1 20021018 17:02:56 20020809 14:00:00 -7e-04    20021017    17:04:00\n2 20021008 13:19:30 20020812 10:54:00 -7e-04    20021017    17:04:00\n3 20021018 15:09:01 20020812 12:13:00 -7e-04    20021017    17:04:00\n4 20021018 16:46:17 20021009  8:50:00 -7e-04    20021017    17:04:00\n5 20021129 11:28:32 20020805 13:47:28 -5e-03    20030115    17:39:00\n6 20021202 11:56:50 20021202 11:56:50 -5e-03    20030115    17:39:00\n\n\n\n\nThe first row in NFLX data set: On 09‐Aug-2002 (ANNDATS), analyst 6749 (ANALYS) at Estimator 1872 (ESTIMATOR) predicts that the EPS (MEASURE) for NETFLIX INC. (CNAME) with a ticker of NFLX (TICKER) with forecast period ending 30‐Sep-2002 (FPEDATS) is -$0.0086 (VALUE). This estimates was entered into the database on 12‐Aug-2002 (ACTDATS). On 17-Oct-2002 (ANNDATS_ACT), NETFLIX INC. announced an actual EPS of $7e-04 ($0.0007) (ACTUAL) for this quarter (FPI=6).\n\n\nCode\nhead(NFLX,n=1)\n\n\n  TICKER        CNAME  ACTDATS ESTIMATOR ANALYS FPI MEASURE   VALUE  FPEDATS\n1   NFLX NETFLIX INC. 20020812      1872   6749   6     EPS -0.0086 20020930\n   REVDATS  REVTIMS  ANNDATS  ANNTIMS ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1 20021018 17:02:56 20020809 14:00:00 -7e-04    20021017    17:04:00\n\n\n\n\n\n\n\n\n\n\n\nTask 1A: Calculate Missingness\n\n\n\nCheck to see the missing values in NFLX dataset and calculate the percent missing for each variable in NFLX and list your findings in R object called NFLX_missingness. NFLX_missingness is a dataframe with two columns: The first column, Variable, stores the variable names and the second column, Missingness shows the percent missing in percentage points with two decimal points.\n\n\n\n\n\n\n\nCode\n# Read CSV file\nNFLX &lt;- fread(\"C:/Users/HP/Downloads/NFLX.csv\")\n\n# Calculate missingness manually\nmissingness_percentages &lt;- colMeans(is.na(NFLX)) * 100\n\n# Create a data frame with Variable names and MissingPercentage values\nmissing_data &lt;- data.frame(Variable = names(missingness_percentages), MissingPercentage = missingness_percentages)\n\n# Print missingness percentages\nprint(\"Missing Values Percentage by Variable\")\n\n\n[1] \"Missing Values Percentage by Variable\"\n\n\nCode\nprint(missing_data)\n\n\n               Variable MissingPercentage\nTICKER           TICKER          0.000000\nCNAME             CNAME          0.000000\nACTDATS         ACTDATS          0.000000\nESTIMATOR     ESTIMATOR          0.000000\nANALYS           ANALYS          0.000000\nFPI                 FPI          0.000000\nMEASURE         MEASURE          0.000000\nVALUE             VALUE          0.000000\nFPEDATS         FPEDATS          0.000000\nREVDATS         REVDATS          0.000000\nREVTIMS         REVTIMS          0.000000\nANNDATS         ANNDATS          0.000000\nANNTIMS         ANNTIMS          0.000000\nACTUAL           ACTUAL          4.115304\nANNDATS_ACT ANNDATS_ACT          4.115304\nANNTIMS_ACT ANNTIMS_ACT          0.000000\n\n\nCode\n# Visualize missing values with custom color\nmissingness_plot &lt;- function(data) {\n  ggplot(data, aes(x = reorder(Variable, -MissingPercentage), y = MissingPercentage)) +\n    geom_bar(stat = \"identity\", fill = \"#66c2a5\") +  # Custom color: greenish\n    labs(title = \"Missing Values Percentage by Variable\", \n         x = \"Variable\", y = \"Missing Percentage\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\n\n# Generate and display missing values plot\nmissingness_plot(missing_data)\n\n\n\n\n\n\n\n\n\n\n\nTask 1B: Data Manipulation\n\n\n\nConduct the following data manipulations on NFLX:\n\nDrop rows from the data set when a variable has a missing value\nDrop rows from the data set the quarterly forecasts (drop FPI=6)\nDeclare TICKER, CNAME, ESTIMATOR , ANALYS, FPI , and MEASURE variables as factor\nDeclare ACTDATS, FPEDATS , ANNDATS, REVDATS, ANNDATS_ACT as time variable.\nDrop ANNTIMS_ACT, ANNTIMS , and REVTIMS\nCreate a new column named YEAR that captures the year in FPEDATS\nName your reduced dataset as NFLX1\nPrint out data structure and the summary of NFLX1\n\n\n\n\n\n\n\n\nCode\n# Copy NFLX to NFLX1 without assigning data types\nNFLX1 &lt;- NFLX\n\n# Drop rows from the data set when a variable has a missing value\nNFLX1 &lt;- NFLX1 %&gt;% na.omit()\n\n# Drop rows from the data set where FPI=6\nNFLX1 &lt;- NFLX1 %&gt;% filter(FPI != 6)\n\n# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS\nNFLX1 &lt;- NFLX1 %&gt;% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)\n\n# Create a new column named YEAR that is an exact copy of the data in FPEDATS\nNFLX1 &lt;- NFLX1 %&gt;% mutate(YEAR = FPEDATS)\n\n# Print out data structure and the summary of NFLX1\nstr(NFLX1)\n\n\nClasses 'data.table' and 'data.frame':  2603 obs. of  14 variables:\n $ TICKER     : chr  \"NFLX\" \"NFLX\" \"NFLX\" \"NFLX\" ...\n $ CNAME      : chr  \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" ...\n $ ACTDATS    : int  20020805 20021202 20021202 20021202 20021205 20030106 20030115 20030116 20030121 20030314 ...\n $ ESTIMATOR  : int  183 2178 1872 220 2178 1872 2227 220 1872 481 ...\n $ ANALYS     : int  79868 80485 6749 57596 80485 6749 82629 57596 6749 81599 ...\n $ FPI        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ MEASURE    : chr  \"EPS\" \"EPS\" \"EPS\" \"EPS\" ...\n $ VALUE      : num  -0.025 -0.0321 -0.0207 -0.0179 -0.0286 -0.0136 -0.0164 -0.0071 0.0107 0.0129 ...\n $ FPEDATS    : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n $ REVDATS    : int  20021129 20021202 20021202 20021206 20021205 20030114 20030115 20030417 20030402 20030409 ...\n $ ANNDATS    : int  20020805 20021202 20021202 20021202 20021204 20030102 20030115 20030116 20030116 20030314 ...\n $ ACTUAL     : num  -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 0.0393 0.0393 0.0393 ...\n $ ANNDATS_ACT: int  20030115 20030115 20030115 20030115 20030115 20030115 20030115 20040121 20040121 20040121 ...\n $ YEAR       : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nCode\nsummary(NFLX1)\n\n\n    TICKER             CNAME              ACTDATS           ESTIMATOR   \n Length:2603        Length:2603        Min.   :20020805   Min.   :  11  \n Class :character   Class :character   1st Qu.:20101021   1st Qu.: 192  \n Mode  :character   Mode  :character   Median :20141009   Median : 899  \n                                       Mean   :20136831   Mean   :1376  \n                                       3rd Qu.:20180122   3rd Qu.:2502  \n                                       Max.   :20210119   Max.   :4439  \n     ANALYS            FPI      MEASURE              VALUE       \n Min.   :  1047   Min.   :1   Length:2603        Min.   :-0.150  \n 1st Qu.: 71755   1st Qu.:1   Class :character   1st Qu.: 0.190  \n Median : 82010   Median :1   Mode  :character   Median : 0.430  \n Mean   : 89534   Mean   :1                      Mean   : 1.339  \n 3rd Qu.:114459   3rd Qu.:1                      3rd Qu.: 2.015  \n Max.   :194536   Max.   :1                      Max.   : 7.670  \n    FPEDATS            REVDATS            ANNDATS             ACTUAL      \n Min.   :20021231   Min.   :20021129   Min.   :20020805   Min.   :-0.005  \n 1st Qu.:20101231   1st Qu.:20110120   1st Qu.:20101021   1st Qu.: 0.250  \n Median :20141231   Median :20141013   Median :20141009   Median : 0.430  \n Mean   :20137082   Mean   :20137740   Mean   :20136830   Mean   : 1.384  \n 3rd Qu.:20181231   3rd Qu.:20180122   3rd Qu.:20180122   3rd Qu.: 2.680  \n Max.   :20201231   Max.   :20210119   Max.   :20210119   Max.   : 6.080  \n  ANNDATS_ACT            YEAR         \n Min.   :20030115   Min.   :20021231  \n 1st Qu.:20110126   1st Qu.:20101231  \n Median :20150120   Median :20141231  \n Mean   :20145973   Mean   :20137082  \n 3rd Qu.:20190117   3rd Qu.:20181231  \n Max.   :20210119   Max.   :20201231  \n\n\n\n\n\n\n\n\nTask 2: Calculate Number of Analysts and Brokerage Houses\n\n\n\n\nCalculate the total number of unique analysts in NFLX1 dataset that provide forecasts each year and name your R object as NumberAnalyst\nCalculate the total number of unique brokerage houses (ESTIMATOR) in NFLX1 dataset that provide forecasts each year and name your R object as NumberBrokerage\nNeed Written Response in this callout: In which year(s) we have the highest number of unique analysts providing forecasts for NFLX ticker? In which year(s), we have the highest number of unique brokerage houses providing forecasts for the NFLX ticker.\n\n2020 is the year December 31 is the month which had the most unique number of analyst providing forecast for NTFLX.\n\n\n\n\n\n\n\nCode\n# Create a new column named YEAR\nNFLX1[, YEAR := format(FPEDATS)]\n\n# Calculate unique analysts each year\nNumberAnalyst &lt;- NFLX1[, .(NumAnalysts = uniqueN(ANALYS)), by = YEAR]\n\n# Print NumberAnalyst object\nprint(NumberAnalyst)\n\n\n        YEAR NumAnalysts\n 1: 20021231           5\n 2: 20031231           9\n 3: 20041231          19\n 4: 20051231          17\n 5: 20061231          20\n 6: 20071231          20\n 7: 20081231          20\n 8: 20091231          33\n 9: 20101231          37\n10: 20111231          40\n11: 20121231          38\n12: 20131231          42\n13: 20141231          45\n14: 20151231          47\n15: 20161231          46\n16: 20171231          48\n17: 20181231          56\n18: 20191231          46\n19: 20201231          49\n\n\nCode\n# Calculate unique brokerage houses each year\nNumberBrokerage &lt;- NFLX1[, .(NumBrokerage = uniqueN(ESTIMATOR)), by = YEAR]\n\n# Print NumberBrokerage object\nprint(NumberBrokerage)\n\n\n        YEAR NumBrokerage\n 1: 20021231            5\n 2: 20031231            8\n 3: 20041231           18\n 4: 20051231           17\n 5: 20061231           19\n 6: 20071231           18\n 7: 20081231           21\n 8: 20091231           32\n 9: 20101231           38\n10: 20111231           35\n11: 20121231           36\n12: 20131231           43\n13: 20141231           40\n14: 20151231           46\n15: 20161231           45\n16: 20171231           49\n17: 20181231           54\n18: 20191231           43\n19: 20201231           44\n\n\n\n\n\n\n\n\nTask 3: Get the most recent forecast in each year\n\n\n\n\nIt is quite possible that an analyst makes multiple forecasts throughout the year for the same fiscal period. Remove observations from NFLX1 if an analyst has multiple predictions for the same year and keep the last one (the most recent forecast for each year). Name your new dataset as NFLX2. This step is crucial for successful execution of the following tasks. Print the dimension of NFLX2.\nCheck your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!\n\n\n\n\n\n\n\n\nCode\n# Task 3: Get the most recent forecast in each year\n# Get most recent forecasts\nNFLX2 &lt;- NFLX1 %&gt;%\n    group_by(ANALYS, YEAR) %&gt;%\n    filter(REVDATS == max(REVDATS)) %&gt;%\n    ungroup()\n\n# Get dimensions\ndim(NFLX2)\n\n\n[1] 641  14\n\n\n\n\n\n\n\n\n\n\nTask 4: Calculate past accuracy\n\n\n\n\nCreate a copy of NFLX2 and call it NFLX3\nFor every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy. In the calculation of forecast performance, you can use the VALUE-ACTUAL as the forecast accuracy measure.\nFor each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nAs an example, consider the year 2006, where analyst 1047, employed at brokerage house 464, provided an estimated end-of-period EPS of 0.0929 (VALUE). However, the actual EPS for that year turned out to be 0.1014 (ACTUAL), resulting in a forecasting error of -0.0085. Consequently, in the subsequent year, 2007, the past_accuracy metric for analyst 1047 would reflect this error by taking the value of -0.0085 (VALUE-ACTUAL).\nThis action will create some missing values and this is perfectly fine.\nIf your code produces 144 NAs, then you are on the right track.\nNote that we are creating copies of the original dataset at each step to facilitate error detection in case any mistakes occur during the process.\n\n\n\n\n\n\n\n\n\nCode\n# Create copy\nNFLX3 &lt;- NFLX2\n\n# Calculate accuracy\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ANALYS) %&gt;%\n  mutate(accuracy = VALUE - ACTUAL) %&gt;%\n  group_by(ANALYS) %&gt;%\n  arrange(YEAR) %&gt;%\n  mutate(past_accuracy = lag(accuracy))\n\n# Check NAs\nsum(is.na(NFLX3$past_accuracy))\n\n\n[1] 144\n\n\n\n\n\n\n\n\n\n\nTask 5: Forecast Horizon\n\n\n\n\nThe longer the forecast horizon, the higher the uncertainty associated with EPS forecasts. To control for this fact, create a new column in NFLX3 called horizon that captures the forecast horizon (ANNDATS_ACT- ANNDATS) for each analyst.\nWe anticipate observing a negative correlation between accuracy and horizon. Typically, as the forecast horizon increases, the accuracy tends to decrease, and vice versa. However, in our dataset, there is an exception where we find a positive correlation between accuracy and horizon for one specific year. Write an R code to identify and determine which year exhibits this positive correlation.\nNeed Written Response in this callout: Enter the year in here.\n2018-11-17 with a correlation value of 0.24300105\n\n\n\n\n\n\n\n\n\nCode\n# Task 5: Forecast Horizon\n# Calculate horizon\nNFLX3 &lt;- NFLX3 %&gt;% mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = \"days\")))\n\n# Calculate correlation\ncorrelation_by_year &lt;- NFLX3 %&gt;% group_by(YEAR) %&gt;% summarise(correlation = cor(accuracy, horizon, use = \"complete.obs\"))\n\n# Convert YEAR\ncorrelation_by_year$YEAR &lt;- format(as.POSIXlt(correlation_by_year$YEAR, format = \"%Y\"), \"%Y-%m-%d %H:%M:%S\")\n\n# Find positive correlation\npositive_corr_year &lt;- correlation_by_year %&gt;% filter(correlation &gt; 0) %&gt;% arrange(desc(correlation))\n\n# Print positive correlation year and correlation value\nprint(paste(positive_corr_year$YEAR, positive_corr_year$correlation, sep = \" \"))\n\n\n[1] \"2018-12-06 00:00:00 0.243001051264513\" \n[2] \"2011-12-06 00:00:00 0.226647459261001\" \n[3] \"2012-12-06 00:00:00 0.0653612459718447\"\n[4] \"2015-12-06 00:00:00 0.0558201596685652\"\n[5] \"2013-12-06 00:00:00 0.0213974532265298\"\n\n\n\n\n\n\n\n\n\n\nTable 6: Experience\n\n\n\n\nWe assume that if an analyst is monitoring a company for a long period of time, he/she is expected to make more informed predictions. Create a new column in NFLX3 called experience that counts the cumulative number of years the analyst monitor (have predictions) the company. Print the summary of experience column.\nHint: Try to use cumsum() function in R.\nNeed Written Response in this callout: Which analyst (s) has the highest number of experience in NFLX3 dataset and for how long do they monitor the NFLX ticker?\n\nTwo unique analyst with the following identifiers 72088 and 77748 had an hardcore experience of 17 years observing the NTFLX stock and making predictions based on the data provided.\n\n\n\n\n\n\n\n\nCode\n# Task 6: Experience\n# Calculate cumulative experience\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  mutate(experience = cumsum(!duplicated(YEAR)))\n\n# Find analyst(s) with the highest experience\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience)) %&gt;%\n  arrange(desc(experience))\n\n# Print summary of the experience column\nsummary(NFLX3$experience)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.192   6.000  17.000 \n\n\nCode\n# Print analyst(s) with the highest experience\nprint(max_experience)\n\n\n# A tibble: 2 × 2\n  ANALYS experience\n   &lt;int&gt;      &lt;int&gt;\n1  72088         17\n2  77748         17\n\n\nCode\n# Define custom color palette\ncustom_colors &lt;- c(\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\", \"#ffd92f\", \"#e5c494\", \"#b3b3b3\")\n\n# Visualization with custom colors\nggplot(data = max_experience, aes(x = reorder(ANALYS, -experience), y = experience, fill = reorder(ANALYS, -experience))) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Analyst\", y = \"Cumulative Experience\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = custom_colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 7: Size\n\n\n\n\nIf a brokerage house has multiple analysts providing predictions for the same company, it may indicate a greater allocation of resources for company analysis. To capture this, create a new column in the NFLX3 dataset called size that calculates the total count of unique analysts employed per year by each brokerage house (ESTIMATOR)\nNeed Written Response in this callout: Print the frequencies for size variable. What does this frequency table reveal about the distribution of the number of analysts hired by brokerage houses in this dataset?\n\nWe concluded that the number of analysts increase the frequency of hiring reduces exponentially. This indicates that with time the brokerage firm decided to hire an experience analyst per season to maximize productivity.\n\n\n\n\n\n\n\n\nCode\n # Task 7: Size\n# Calculate analysts per year\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ESTIMATOR) %&gt;%\n  mutate(size = n_distinct(ANALYS))\n\n# Print frequencies\nsize_freq &lt;- table(NFLX3$size)\nprint(size_freq)\n\n\n\n  1   2   3 \n560  72   9 \n\n\nCode\n# Create frequency table\nsize_table &lt;- as.data.frame(size_freq)\ncolnames(size_table) &lt;- c(\"Analysts\", \"Frequency\")\n\n# Sort by frequency\nsize_table &lt;- size_table[order(-size_table$Frequency), ]\n\n# Print sorted table using kableExtra\nsize_table %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\n\n\nAnalysts\nFrequency\n\n\n\n\n1\n560\n\n\n2\n72\n\n\n3\n9\n\n\n\n\n\n\n\nCode\n# Summary statistics\nsummary(NFLX3$size)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    1.00    1.14    1.00    3.00 \n\n\nCode\n# Visualize results with different colors\nbarplot(size_table$Frequency, names.arg = size_table$Analysts,\n        xlab = \"Analysts\", ylab = \"Frequency\",\n        main = \"Analysts by Brokerage\",\n        col = c(\"skyblue\", \"salmon\", \"lightgreen\"), border = \"black\",\n        las = 1, cex.names = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 8: Prediction 1\n\n\n\n\nIn the year 2020, NETFLIX reported an actual earnings per share (EPS) of $6.08. To predict this EPS value based on historical data, we will employ a linear regression model using the dataset NFLX3 up until the year 2019. In this model, the target variable will be ACTUAL and the predictor variables will include VALUE and past_accuracy. C.all your model as model1.\nNeed Written Response in this callout: Using the linear regression model ‘model1,’ which has been trained on historical data up to the year 2019, what is the forecasted EPS (Earnings Per Share) for the year 2020? Please provide a brief explanation of the method you employed to make this prediction. If you encountered any challenges or were unable to make the calculation, briefly describe the specific issues you encountered.\n\nUtilizing the linear regression model ‘model1’ trained on historical data until 2019, we can predict the Earnings Per Share (EPS) for 2020. The process involves assessing the model’s goodness of fit, measured by the R-squared value. If the R-squared value surpasses 0.5, indicating a strong fit, the code calculates the mean of the ‘past_accuracy’ variable. When the fit is satisfactory, the model generates a forecast for the future period. This involves creating a new dataset with appropriate values for independent variables (VALUE and past_accuracy) and utilizing the ‘predict’ function to estimate the EPS. Conversely, if the R-squared value falls below the threshold, a warning message alerts that the model might not accurately predict future EPS. Addressing specific challenges related to data configuration or model training is essential to ensure the forecast’s accuracy. This method enables accurate predictions under favorable model fit conditions, ensuring reliable forecasting outcomes.\n\n\n\n\n\n\n\n\nCode\n# Calculate mean past_accuracy\nmean_past_accuracy &lt;- mean(NFLX3$past_accuracy, na.rm = TRUE)\n\n\n# Create linear regression model\nmodel1 &lt;- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)\n\n# Get R-squared value\nr_squared &lt;- summary(model1)$r.squared\n\n# Check R-squared\nif (r_squared &gt; 0.5) {\n\n  # Create new data frame for future period\n  new_data_future &lt;- data.frame(\n    VALUE = 6.08,\n    past_accuracy = mean_past_accuracy\n  )\n\n  # Predict EPS for future\n  predicted_eps_future &lt;- predict(model1, newdata = new_data_future)\n\n  # Print forecasted EPS\n  cat(\"Forecasted EPS: $\", round(predicted_eps_future, 2))\n\n} else {\n\n  # Print warning\n  cat(\"Low R-squared value.\")\n\n}\n\n\nForecasted EPS: $ 6.3\n\n\nCode\n# Print mean past_accuracy\ncat(\"Mean past_accuracy: \", round(mean_past_accuracy, 2))\n\n\nMean past_accuracy:  -0.09\n\n\n\n\n\n\n\n\n\n\nTask 9: Prediction 2\n\n\n\n\nAs an alternative approach, instead of modeling the ‘ACTUAL’ value, we can obtain the mean and median forecasts for the year 2020 as our best estimates of the EPS value for that year.\nNeed Written Response in this callout: Please calculate these forecasts and then compare them with the results from the previous task. Finally, provide your insights and comments based on your findings.\n\nIn this alternative approach, we calculated both the mean and median forecasts for the year 2020 to estimate the EPS value. The mean forecast stood at approximately $1.24, whereas the median forecast was notably lower, around $0.41. Comparing these outcomes with the linear regression model used earlier, it becomes evident that the model-based forecast offers a more detailed and potentially accurate prediction. However, each method has its distinct advantages and disadvantages. The model-driven forecast considers historical relationships and variables like ‘past_accuracy,’ but its accuracy heavily depends on the quality of the model fit, indicated by the R-squared value. On the contrary, the mean and median forecasts provide straightforward summary statistics but might lack the predictive strength of a well-fitted model. The choice between these methods should be made considering the data quality and the specific context of the analysis.\n\n\n\n\n\n\n\n\nCode\n# Calculate mean forecast\nmean_forecast &lt;- mean(NFLX3$VALUE, na.rm = TRUE)\n\n# Calculate median forecast\nmedian_forecast &lt;- median(NFLX3$VALUE, na.rm = TRUE)\n\n# Print forecasts\ncat(\"Mean forecast: $\", round(mean_forecast, 2))\n\n\nMean forecast: $ 1.24\n\n\nCode\ncat(\"Median forecast: $\", round(median_forecast, 2))\n\n\nMedian forecast: $ 0.41\n\n\n\n\n\n\n\n\n\n\nTask 10: Averages\n\n\n\n\nGenerate a new dataset named NFLX4 by aggregating data from NFLX3 Include the variables size, experience, horizon, accuracy, past_accuracy, and ACTUAL in NFLX4. When calculating the yearly averages for these variables, ignore any missing values (NAs). Present a summary of the NFLX4 dataset.\nNeed Written Response in this callout: Subsequently, employ correlation analysis or exploratory data analysis to get insights into the relationships between these variables and ‘ACTUAL,’ if such relationships exist.\n\nFollowing an in-depth analysis of the NFLX4 dataset, significant patterns concerning the relationship between variables and 'ACTUAL' earnings per share have been uncovered. The correlation analysis indicated that 'ACTUAL' is positively linked with 'size' (0.18) and 'experience' (0.69), implying that larger and more experienced analyst groups tend to deliver more precise forecasts for 'ACTUAL'. Conversely, 'ACTUAL' exhibits negative correlations with 'horizon' (-0.63) and 'past_accuracy' (-0.80), indicating that analysts with longer forecasting periods and higher past accuracy often produce less accurate predictions for 'ACTUAL'. Visual representations through scatter plots reinforce these findings, such as the trend showing enhanced accuracy with an increase in the number of analysts in the 'ACTUAL' versus 'size' plot. These insights shed light on the factors influencing earnings per share forecasts, underscoring the importance of analyst group size and experience while emphasizing the potential drawbacks of longer forecasting horizons and overly accurate track records, crucial knowledge for financial analysts, investors, and decision-makers.\n\n\n\n\nCode\n# Aggregate and calculate averages\nNFLX4 &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    size = mean(size, na.rm = TRUE),\n    experience = mean(experience, na.rm = TRUE),\n    horizon = mean(horizon, na.rm = TRUE),\n    accuracy = mean(accuracy, na.rm = TRUE),\n    past_accuracy = mean(past_accuracy, na.rm = TRUE),\n    ACTUAL = mean(ACTUAL, na.rm = TRUE)\n  )\n\n# Summary of NFLX4 dataset\nsummary(NFLX4)\n\n\n     YEAR                size         experience       horizon       \n Length:19          Min.   :1.000   Min.   :1.000   Min.   :0.06284  \n Class :character   1st Qu.:1.074   1st Qu.:2.664   1st Qu.:0.08547  \n Mode  :character   Median :1.105   Median :3.400   Median :0.09289  \n                    Mean   :1.132   Mean   :3.611   Mean   :0.09004  \n                    3rd Qu.:1.202   3rd Qu.:4.869   3rd Qu.:0.09512  \n                    Max.   :1.300   Max.   :6.061   Max.   :0.10656  \n                                                                     \n    accuracy         past_accuracy           ACTUAL       \n Min.   :-0.822085   Min.   :-0.798219   Min.   :-0.0050  \n 1st Qu.:-0.019087   1st Qu.:-0.028736   1st Qu.: 0.0914  \n Median :-0.015035   Median :-0.013423   Median : 0.2643  \n Mean   :-0.048310   Mean   :-0.060652   Mean   : 0.9248  \n 3rd Qu.:-0.005415   3rd Qu.:-0.009260   3rd Qu.: 0.5678  \n Max.   : 0.121449   Max.   :-0.001547   Max.   : 6.0800  \n                     NA's   :1                            \n\n\nCode\n# Create a correlation matrix\ncorrelation_matrix &lt;- cor(NFLX4[, c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\")], use = \"complete.obs\")\n\n\n# Custom color palette\ncolor_palette &lt;- colorRampPalette(c(\"#FFFFFF\", \"#67a9cf\", \"#ef8a62\", \"#b2182b\"))(100)\n\n# Create a correlation plot with enhanced customization\ncorrplot(\n  correlation_matrix,\n  method = \"color\",\n  col = color_palette,\n  type = \"upper\",\n  order = \"original\",\n  tl.cex = 0.7,  # Adjust the size of text labels\n  cl.cex = 0.8,  # Adjust the size of correlation coefficients\n  diag = FALSE,\n  number.cex = 0.8\n)\n\n\n\n\n\nCode\n# Print correlation matrix\nprint(correlation_matrix)\n\n\n                     size  experience    horizon    accuracy past_accuracy\nsize           1.00000000  0.07451284 -0.1317823 -0.04537307    -0.1810330\nexperience     0.07451284  1.00000000 -0.4844637 -0.25882136    -0.4620906\nhorizon       -0.13178225 -0.48446371  1.0000000  0.22264895     0.4979377\naccuracy      -0.04537307 -0.25882136  0.2226489  1.00000000    -0.1604379\npast_accuracy -0.18103301 -0.46209061  0.4979377 -0.16043792     1.0000000\nACTUAL         0.18223220  0.68707354 -0.6346966 -0.31928984    -0.7958850\n                  ACTUAL\nsize           0.1822322\nexperience     0.6870735\nhorizon       -0.6346966\naccuracy      -0.3192898\npast_accuracy -0.7958850\nACTUAL         1.0000000\n\n\nCode\nggpairs(NFLX4, columns = c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\"))\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = size)) +\n  geom_histogram(fill = \"#009999\", color = \"black\", bins = 20) +\n  ggtitle(\"Distribution of Size\") +\n  xlab(\"Size\") +\n  ylab(\"Frequency\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(NFLX4, aes(y = size)) +\n  geom_boxplot(fill = \"#009999\") +\n  ggtitle(\"Boxplot of Size\") +\n  ylab(\"Size\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/final project/index.html#how-to-read-the-data",
    "href": "posts/final project/index.html#how-to-read-the-data",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "The first row in NFLX data set: On 09‐Aug-2002 (ANNDATS), analyst 6749 (ANALYS) at Estimator 1872 (ESTIMATOR) predicts that the EPS (MEASURE) for NETFLIX INC. (CNAME) with a ticker of NFLX (TICKER) with forecast period ending 30‐Sep-2002 (FPEDATS) is -$0.0086 (VALUE). This estimates was entered into the database on 12‐Aug-2002 (ACTDATS). On 17-Oct-2002 (ANNDATS_ACT), NETFLIX INC. announced an actual EPS of $7e-04 ($0.0007) (ACTUAL) for this quarter (FPI=6).\n\n\nCode\nhead(NFLX,n=1)\n\n\n  TICKER        CNAME  ACTDATS ESTIMATOR ANALYS FPI MEASURE   VALUE  FPEDATS\n1   NFLX NETFLIX INC. 20020812      1872   6749   6     EPS -0.0086 20020930\n   REVDATS  REVTIMS  ANNDATS  ANNTIMS ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1 20021018 17:02:56 20020809 14:00:00 -7e-04    20021017    17:04:00"
  },
  {
    "objectID": "posts/final project/index.html#your-turn",
    "href": "posts/final project/index.html#your-turn",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Task 1A: Calculate Missingness\n\n\n\nCheck to see the missing values in NFLX dataset and calculate the percent missing for each variable in NFLX and list your findings in R object called NFLX_missingness. NFLX_missingness is a dataframe with two columns: The first column, Variable, stores the variable names and the second column, Missingness shows the percent missing in percentage points with two decimal points."
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-1a",
    "href": "posts/final project/index.html#your-code-for-task-1a",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Read CSV file\nNFLX &lt;- fread(\"C:/Users/HP/Downloads/NFLX.csv\")\n\n# Calculate missingness manually\nmissingness_percentages &lt;- colMeans(is.na(NFLX)) * 100\n\n# Create a data frame with Variable names and MissingPercentage values\nmissing_data &lt;- data.frame(Variable = names(missingness_percentages), MissingPercentage = missingness_percentages)\n\n# Print missingness percentages\nprint(\"Missing Values Percentage by Variable\")\n\n\n[1] \"Missing Values Percentage by Variable\"\n\n\nCode\nprint(missing_data)\n\n\n               Variable MissingPercentage\nTICKER           TICKER          0.000000\nCNAME             CNAME          0.000000\nACTDATS         ACTDATS          0.000000\nESTIMATOR     ESTIMATOR          0.000000\nANALYS           ANALYS          0.000000\nFPI                 FPI          0.000000\nMEASURE         MEASURE          0.000000\nVALUE             VALUE          0.000000\nFPEDATS         FPEDATS          0.000000\nREVDATS         REVDATS          0.000000\nREVTIMS         REVTIMS          0.000000\nANNDATS         ANNDATS          0.000000\nANNTIMS         ANNTIMS          0.000000\nACTUAL           ACTUAL          4.115304\nANNDATS_ACT ANNDATS_ACT          4.115304\nANNTIMS_ACT ANNTIMS_ACT          0.000000\n\n\nCode\n# Visualize missing values with custom color\nmissingness_plot &lt;- function(data) {\n  ggplot(data, aes(x = reorder(Variable, -MissingPercentage), y = MissingPercentage)) +\n    geom_bar(stat = \"identity\", fill = \"#66c2a5\") +  # Custom color: greenish\n    labs(title = \"Missing Values Percentage by Variable\", \n         x = \"Variable\", y = \"Missing Percentage\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\n\n# Generate and display missing values plot\nmissingness_plot(missing_data)\n\n\n\n\n\n\n\n\n\n\n\nTask 1B: Data Manipulation\n\n\n\nConduct the following data manipulations on NFLX:\n\nDrop rows from the data set when a variable has a missing value\nDrop rows from the data set the quarterly forecasts (drop FPI=6)\nDeclare TICKER, CNAME, ESTIMATOR , ANALYS, FPI , and MEASURE variables as factor\nDeclare ACTDATS, FPEDATS , ANNDATS, REVDATS, ANNDATS_ACT as time variable.\nDrop ANNTIMS_ACT, ANNTIMS , and REVTIMS\nCreate a new column named YEAR that captures the year in FPEDATS\nName your reduced dataset as NFLX1\nPrint out data structure and the summary of NFLX1"
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-1b",
    "href": "posts/final project/index.html#your-code-for-task-1b",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Copy NFLX to NFLX1 without assigning data types\nNFLX1 &lt;- NFLX\n\n# Drop rows from the data set when a variable has a missing value\nNFLX1 &lt;- NFLX1 %&gt;% na.omit()\n\n# Drop rows from the data set where FPI=6\nNFLX1 &lt;- NFLX1 %&gt;% filter(FPI != 6)\n\n# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS\nNFLX1 &lt;- NFLX1 %&gt;% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)\n\n# Create a new column named YEAR that is an exact copy of the data in FPEDATS\nNFLX1 &lt;- NFLX1 %&gt;% mutate(YEAR = FPEDATS)\n\n# Print out data structure and the summary of NFLX1\nstr(NFLX1)\n\n\nClasses 'data.table' and 'data.frame':  2603 obs. of  14 variables:\n $ TICKER     : chr  \"NFLX\" \"NFLX\" \"NFLX\" \"NFLX\" ...\n $ CNAME      : chr  \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" ...\n $ ACTDATS    : int  20020805 20021202 20021202 20021202 20021205 20030106 20030115 20030116 20030121 20030314 ...\n $ ESTIMATOR  : int  183 2178 1872 220 2178 1872 2227 220 1872 481 ...\n $ ANALYS     : int  79868 80485 6749 57596 80485 6749 82629 57596 6749 81599 ...\n $ FPI        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ MEASURE    : chr  \"EPS\" \"EPS\" \"EPS\" \"EPS\" ...\n $ VALUE      : num  -0.025 -0.0321 -0.0207 -0.0179 -0.0286 -0.0136 -0.0164 -0.0071 0.0107 0.0129 ...\n $ FPEDATS    : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n $ REVDATS    : int  20021129 20021202 20021202 20021206 20021205 20030114 20030115 20030417 20030402 20030409 ...\n $ ANNDATS    : int  20020805 20021202 20021202 20021202 20021204 20030102 20030115 20030116 20030116 20030314 ...\n $ ACTUAL     : num  -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 0.0393 0.0393 0.0393 ...\n $ ANNDATS_ACT: int  20030115 20030115 20030115 20030115 20030115 20030115 20030115 20040121 20040121 20040121 ...\n $ YEAR       : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nCode\nsummary(NFLX1)\n\n\n    TICKER             CNAME              ACTDATS           ESTIMATOR   \n Length:2603        Length:2603        Min.   :20020805   Min.   :  11  \n Class :character   Class :character   1st Qu.:20101021   1st Qu.: 192  \n Mode  :character   Mode  :character   Median :20141009   Median : 899  \n                                       Mean   :20136831   Mean   :1376  \n                                       3rd Qu.:20180122   3rd Qu.:2502  \n                                       Max.   :20210119   Max.   :4439  \n     ANALYS            FPI      MEASURE              VALUE       \n Min.   :  1047   Min.   :1   Length:2603        Min.   :-0.150  \n 1st Qu.: 71755   1st Qu.:1   Class :character   1st Qu.: 0.190  \n Median : 82010   Median :1   Mode  :character   Median : 0.430  \n Mean   : 89534   Mean   :1                      Mean   : 1.339  \n 3rd Qu.:114459   3rd Qu.:1                      3rd Qu.: 2.015  \n Max.   :194536   Max.   :1                      Max.   : 7.670  \n    FPEDATS            REVDATS            ANNDATS             ACTUAL      \n Min.   :20021231   Min.   :20021129   Min.   :20020805   Min.   :-0.005  \n 1st Qu.:20101231   1st Qu.:20110120   1st Qu.:20101021   1st Qu.: 0.250  \n Median :20141231   Median :20141013   Median :20141009   Median : 0.430  \n Mean   :20137082   Mean   :20137740   Mean   :20136830   Mean   : 1.384  \n 3rd Qu.:20181231   3rd Qu.:20180122   3rd Qu.:20180122   3rd Qu.: 2.680  \n Max.   :20201231   Max.   :20210119   Max.   :20210119   Max.   : 6.080  \n  ANNDATS_ACT            YEAR         \n Min.   :20030115   Min.   :20021231  \n 1st Qu.:20110126   1st Qu.:20101231  \n Median :20150120   Median :20141231  \n Mean   :20145973   Mean   :20137082  \n 3rd Qu.:20190117   3rd Qu.:20181231  \n Max.   :20210119   Max.   :20201231  \n\n\n\n\n\n\n\n\nTask 2: Calculate Number of Analysts and Brokerage Houses\n\n\n\n\nCalculate the total number of unique analysts in NFLX1 dataset that provide forecasts each year and name your R object as NumberAnalyst\nCalculate the total number of unique brokerage houses (ESTIMATOR) in NFLX1 dataset that provide forecasts each year and name your R object as NumberBrokerage\nNeed Written Response in this callout: In which year(s) we have the highest number of unique analysts providing forecasts for NFLX ticker? In which year(s), we have the highest number of unique brokerage houses providing forecasts for the NFLX ticker.\n\n2020 is the year December 31 is the month which had the most unique number of analyst providing forecast for NTFLX."
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-2",
    "href": "posts/final project/index.html#your-code-for-task-2",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Create a new column named YEAR\nNFLX1[, YEAR := format(FPEDATS)]\n\n# Calculate unique analysts each year\nNumberAnalyst &lt;- NFLX1[, .(NumAnalysts = uniqueN(ANALYS)), by = YEAR]\n\n# Print NumberAnalyst object\nprint(NumberAnalyst)\n\n\n        YEAR NumAnalysts\n 1: 20021231           5\n 2: 20031231           9\n 3: 20041231          19\n 4: 20051231          17\n 5: 20061231          20\n 6: 20071231          20\n 7: 20081231          20\n 8: 20091231          33\n 9: 20101231          37\n10: 20111231          40\n11: 20121231          38\n12: 20131231          42\n13: 20141231          45\n14: 20151231          47\n15: 20161231          46\n16: 20171231          48\n17: 20181231          56\n18: 20191231          46\n19: 20201231          49\n\n\nCode\n# Calculate unique brokerage houses each year\nNumberBrokerage &lt;- NFLX1[, .(NumBrokerage = uniqueN(ESTIMATOR)), by = YEAR]\n\n# Print NumberBrokerage object\nprint(NumberBrokerage)\n\n\n        YEAR NumBrokerage\n 1: 20021231            5\n 2: 20031231            8\n 3: 20041231           18\n 4: 20051231           17\n 5: 20061231           19\n 6: 20071231           18\n 7: 20081231           21\n 8: 20091231           32\n 9: 20101231           38\n10: 20111231           35\n11: 20121231           36\n12: 20131231           43\n13: 20141231           40\n14: 20151231           46\n15: 20161231           45\n16: 20171231           49\n17: 20181231           54\n18: 20191231           43\n19: 20201231           44\n\n\n\n\n\n\n\n\nTask 3: Get the most recent forecast in each year\n\n\n\n\nIt is quite possible that an analyst makes multiple forecasts throughout the year for the same fiscal period. Remove observations from NFLX1 if an analyst has multiple predictions for the same year and keep the last one (the most recent forecast for each year). Name your new dataset as NFLX2. This step is crucial for successful execution of the following tasks. Print the dimension of NFLX2.\nCheck your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!"
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-3",
    "href": "posts/final project/index.html#your-code-for-task-3",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Task 3: Get the most recent forecast in each year\n# Get most recent forecasts\nNFLX2 &lt;- NFLX1 %&gt;%\n    group_by(ANALYS, YEAR) %&gt;%\n    filter(REVDATS == max(REVDATS)) %&gt;%\n    ungroup()\n\n# Get dimensions\ndim(NFLX2)\n\n\n[1] 641  14\n\n\n\n\n\n\n\n\n\n\nTask 4: Calculate past accuracy\n\n\n\n\nCreate a copy of NFLX2 and call it NFLX3\nFor every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy. In the calculation of forecast performance, you can use the VALUE-ACTUAL as the forecast accuracy measure.\nFor each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nAs an example, consider the year 2006, where analyst 1047, employed at brokerage house 464, provided an estimated end-of-period EPS of 0.0929 (VALUE). However, the actual EPS for that year turned out to be 0.1014 (ACTUAL), resulting in a forecasting error of -0.0085. Consequently, in the subsequent year, 2007, the past_accuracy metric for analyst 1047 would reflect this error by taking the value of -0.0085 (VALUE-ACTUAL).\nThis action will create some missing values and this is perfectly fine.\nIf your code produces 144 NAs, then you are on the right track.\nNote that we are creating copies of the original dataset at each step to facilitate error detection in case any mistakes occur during the process."
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-4",
    "href": "posts/final project/index.html#your-code-for-task-4",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Create copy\nNFLX3 &lt;- NFLX2\n\n# Calculate accuracy\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ANALYS) %&gt;%\n  mutate(accuracy = VALUE - ACTUAL) %&gt;%\n  group_by(ANALYS) %&gt;%\n  arrange(YEAR) %&gt;%\n  mutate(past_accuracy = lag(accuracy))\n\n# Check NAs\nsum(is.na(NFLX3$past_accuracy))\n\n\n[1] 144\n\n\n\n\n\n\n\n\n\n\nTask 5: Forecast Horizon\n\n\n\n\nThe longer the forecast horizon, the higher the uncertainty associated with EPS forecasts. To control for this fact, create a new column in NFLX3 called horizon that captures the forecast horizon (ANNDATS_ACT- ANNDATS) for each analyst.\nWe anticipate observing a negative correlation between accuracy and horizon. Typically, as the forecast horizon increases, the accuracy tends to decrease, and vice versa. However, in our dataset, there is an exception where we find a positive correlation between accuracy and horizon for one specific year. Write an R code to identify and determine which year exhibits this positive correlation.\nNeed Written Response in this callout: Enter the year in here.\n2018-11-17 with a correlation value of 0.24300105"
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-5",
    "href": "posts/final project/index.html#your-code-for-task-5",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Task 5: Forecast Horizon\n# Calculate horizon\nNFLX3 &lt;- NFLX3 %&gt;% mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = \"days\")))\n\n# Calculate correlation\ncorrelation_by_year &lt;- NFLX3 %&gt;% group_by(YEAR) %&gt;% summarise(correlation = cor(accuracy, horizon, use = \"complete.obs\"))\n\n# Convert YEAR\ncorrelation_by_year$YEAR &lt;- format(as.POSIXlt(correlation_by_year$YEAR, format = \"%Y\"), \"%Y-%m-%d %H:%M:%S\")\n\n# Find positive correlation\npositive_corr_year &lt;- correlation_by_year %&gt;% filter(correlation &gt; 0) %&gt;% arrange(desc(correlation))\n\n# Print positive correlation year and correlation value\nprint(paste(positive_corr_year$YEAR, positive_corr_year$correlation, sep = \" \"))\n\n\n[1] \"2018-12-06 00:00:00 0.243001051264513\" \n[2] \"2011-12-06 00:00:00 0.226647459261001\" \n[3] \"2012-12-06 00:00:00 0.0653612459718447\"\n[4] \"2015-12-06 00:00:00 0.0558201596685652\"\n[5] \"2013-12-06 00:00:00 0.0213974532265298\"\n\n\n\n\n\n\n\n\n\n\nTable 6: Experience\n\n\n\n\nWe assume that if an analyst is monitoring a company for a long period of time, he/she is expected to make more informed predictions. Create a new column in NFLX3 called experience that counts the cumulative number of years the analyst monitor (have predictions) the company. Print the summary of experience column.\nHint: Try to use cumsum() function in R.\nNeed Written Response in this callout: Which analyst (s) has the highest number of experience in NFLX3 dataset and for how long do they monitor the NFLX ticker?\n\nTwo unique analyst with the following identifiers 72088 and 77748 had an hardcore experience of 17 years observing the NTFLX stock and making predictions based on the data provided."
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-6",
    "href": "posts/final project/index.html#your-code-for-task-6",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Task 6: Experience\n# Calculate cumulative experience\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  mutate(experience = cumsum(!duplicated(YEAR)))\n\n# Find analyst(s) with the highest experience\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience)) %&gt;%\n  arrange(desc(experience))\n\n# Print summary of the experience column\nsummary(NFLX3$experience)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.192   6.000  17.000 \n\n\nCode\n# Print analyst(s) with the highest experience\nprint(max_experience)\n\n\n# A tibble: 2 × 2\n  ANALYS experience\n   &lt;int&gt;      &lt;int&gt;\n1  72088         17\n2  77748         17\n\n\nCode\n# Define custom color palette\ncustom_colors &lt;- c(\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\", \"#ffd92f\", \"#e5c494\", \"#b3b3b3\")\n\n# Visualization with custom colors\nggplot(data = max_experience, aes(x = reorder(ANALYS, -experience), y = experience, fill = reorder(ANALYS, -experience))) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Analyst\", y = \"Cumulative Experience\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = custom_colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 7: Size\n\n\n\n\nIf a brokerage house has multiple analysts providing predictions for the same company, it may indicate a greater allocation of resources for company analysis. To capture this, create a new column in the NFLX3 dataset called size that calculates the total count of unique analysts employed per year by each brokerage house (ESTIMATOR)\nNeed Written Response in this callout: Print the frequencies for size variable. What does this frequency table reveal about the distribution of the number of analysts hired by brokerage houses in this dataset?\n\nWe concluded that the number of analysts increase the frequency of hiring reduces exponentially. This indicates that with time the brokerage firm decided to hire an experience analyst per season to maximize productivity."
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-7",
    "href": "posts/final project/index.html#your-code-for-task-7",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n # Task 7: Size\n# Calculate analysts per year\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ESTIMATOR) %&gt;%\n  mutate(size = n_distinct(ANALYS))\n\n# Print frequencies\nsize_freq &lt;- table(NFLX3$size)\nprint(size_freq)\n\n\n\n  1   2   3 \n560  72   9 \n\n\nCode\n# Create frequency table\nsize_table &lt;- as.data.frame(size_freq)\ncolnames(size_table) &lt;- c(\"Analysts\", \"Frequency\")\n\n# Sort by frequency\nsize_table &lt;- size_table[order(-size_table$Frequency), ]\n\n# Print sorted table using kableExtra\nsize_table %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\n\n\nAnalysts\nFrequency\n\n\n\n\n1\n560\n\n\n2\n72\n\n\n3\n9\n\n\n\n\n\n\n\nCode\n# Summary statistics\nsummary(NFLX3$size)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    1.00    1.14    1.00    3.00 \n\n\nCode\n# Visualize results with different colors\nbarplot(size_table$Frequency, names.arg = size_table$Analysts,\n        xlab = \"Analysts\", ylab = \"Frequency\",\n        main = \"Analysts by Brokerage\",\n        col = c(\"skyblue\", \"salmon\", \"lightgreen\"), border = \"black\",\n        las = 1, cex.names = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 8: Prediction 1\n\n\n\n\nIn the year 2020, NETFLIX reported an actual earnings per share (EPS) of $6.08. To predict this EPS value based on historical data, we will employ a linear regression model using the dataset NFLX3 up until the year 2019. In this model, the target variable will be ACTUAL and the predictor variables will include VALUE and past_accuracy. C.all your model as model1.\nNeed Written Response in this callout: Using the linear regression model ‘model1,’ which has been trained on historical data up to the year 2019, what is the forecasted EPS (Earnings Per Share) for the year 2020? Please provide a brief explanation of the method you employed to make this prediction. If you encountered any challenges or were unable to make the calculation, briefly describe the specific issues you encountered.\n\nUtilizing the linear regression model ‘model1’ trained on historical data until 2019, we can predict the Earnings Per Share (EPS) for 2020. The process involves assessing the model’s goodness of fit, measured by the R-squared value. If the R-squared value surpasses 0.5, indicating a strong fit, the code calculates the mean of the ‘past_accuracy’ variable. When the fit is satisfactory, the model generates a forecast for the future period. This involves creating a new dataset with appropriate values for independent variables (VALUE and past_accuracy) and utilizing the ‘predict’ function to estimate the EPS. Conversely, if the R-squared value falls below the threshold, a warning message alerts that the model might not accurately predict future EPS. Addressing specific challenges related to data configuration or model training is essential to ensure the forecast’s accuracy. This method enables accurate predictions under favorable model fit conditions, ensuring reliable forecasting outcomes."
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-8",
    "href": "posts/final project/index.html#your-code-for-task-8",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Calculate mean past_accuracy\nmean_past_accuracy &lt;- mean(NFLX3$past_accuracy, na.rm = TRUE)\n\n\n# Create linear regression model\nmodel1 &lt;- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)\n\n# Get R-squared value\nr_squared &lt;- summary(model1)$r.squared\n\n# Check R-squared\nif (r_squared &gt; 0.5) {\n\n  # Create new data frame for future period\n  new_data_future &lt;- data.frame(\n    VALUE = 6.08,\n    past_accuracy = mean_past_accuracy\n  )\n\n  # Predict EPS for future\n  predicted_eps_future &lt;- predict(model1, newdata = new_data_future)\n\n  # Print forecasted EPS\n  cat(\"Forecasted EPS: $\", round(predicted_eps_future, 2))\n\n} else {\n\n  # Print warning\n  cat(\"Low R-squared value.\")\n\n}\n\n\nForecasted EPS: $ 6.3\n\n\nCode\n# Print mean past_accuracy\ncat(\"Mean past_accuracy: \", round(mean_past_accuracy, 2))\n\n\nMean past_accuracy:  -0.09\n\n\n\n\n\n\n\n\n\n\nTask 9: Prediction 2\n\n\n\n\nAs an alternative approach, instead of modeling the ‘ACTUAL’ value, we can obtain the mean and median forecasts for the year 2020 as our best estimates of the EPS value for that year.\nNeed Written Response in this callout: Please calculate these forecasts and then compare them with the results from the previous task. Finally, provide your insights and comments based on your findings.\n\nIn this alternative approach, we calculated both the mean and median forecasts for the year 2020 to estimate the EPS value. The mean forecast stood at approximately $1.24, whereas the median forecast was notably lower, around $0.41. Comparing these outcomes with the linear regression model used earlier, it becomes evident that the model-based forecast offers a more detailed and potentially accurate prediction. However, each method has its distinct advantages and disadvantages. The model-driven forecast considers historical relationships and variables like ‘past_accuracy,’ but its accuracy heavily depends on the quality of the model fit, indicated by the R-squared value. On the contrary, the mean and median forecasts provide straightforward summary statistics but might lack the predictive strength of a well-fitted model. The choice between these methods should be made considering the data quality and the specific context of the analysis."
  },
  {
    "objectID": "posts/final project/index.html#your-code-for-task-9",
    "href": "posts/final project/index.html#your-code-for-task-9",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Calculate mean forecast\nmean_forecast &lt;- mean(NFLX3$VALUE, na.rm = TRUE)\n\n# Calculate median forecast\nmedian_forecast &lt;- median(NFLX3$VALUE, na.rm = TRUE)\n\n# Print forecasts\ncat(\"Mean forecast: $\", round(mean_forecast, 2))\n\n\nMean forecast: $ 1.24\n\n\nCode\ncat(\"Median forecast: $\", round(median_forecast, 2))\n\n\nMedian forecast: $ 0.41\n\n\n\n\n\n\n\n\n\n\nTask 10: Averages\n\n\n\n\nGenerate a new dataset named NFLX4 by aggregating data from NFLX3 Include the variables size, experience, horizon, accuracy, past_accuracy, and ACTUAL in NFLX4. When calculating the yearly averages for these variables, ignore any missing values (NAs). Present a summary of the NFLX4 dataset.\nNeed Written Response in this callout: Subsequently, employ correlation analysis or exploratory data analysis to get insights into the relationships between these variables and ‘ACTUAL,’ if such relationships exist.\n\nFollowing an in-depth analysis of the NFLX4 dataset, significant patterns concerning the relationship between variables and 'ACTUAL' earnings per share have been uncovered. The correlation analysis indicated that 'ACTUAL' is positively linked with 'size' (0.18) and 'experience' (0.69), implying that larger and more experienced analyst groups tend to deliver more precise forecasts for 'ACTUAL'. Conversely, 'ACTUAL' exhibits negative correlations with 'horizon' (-0.63) and 'past_accuracy' (-0.80), indicating that analysts with longer forecasting periods and higher past accuracy often produce less accurate predictions for 'ACTUAL'. Visual representations through scatter plots reinforce these findings, such as the trend showing enhanced accuracy with an increase in the number of analysts in the 'ACTUAL' versus 'size' plot. These insights shed light on the factors influencing earnings per share forecasts, underscoring the importance of analyst group size and experience while emphasizing the potential drawbacks of longer forecasting horizons and overly accurate track records, crucial knowledge for financial analysts, investors, and decision-makers.\n\n\n\n\nCode\n# Aggregate and calculate averages\nNFLX4 &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    size = mean(size, na.rm = TRUE),\n    experience = mean(experience, na.rm = TRUE),\n    horizon = mean(horizon, na.rm = TRUE),\n    accuracy = mean(accuracy, na.rm = TRUE),\n    past_accuracy = mean(past_accuracy, na.rm = TRUE),\n    ACTUAL = mean(ACTUAL, na.rm = TRUE)\n  )\n\n# Summary of NFLX4 dataset\nsummary(NFLX4)\n\n\n     YEAR                size         experience       horizon       \n Length:19          Min.   :1.000   Min.   :1.000   Min.   :0.06284  \n Class :character   1st Qu.:1.074   1st Qu.:2.664   1st Qu.:0.08547  \n Mode  :character   Median :1.105   Median :3.400   Median :0.09289  \n                    Mean   :1.132   Mean   :3.611   Mean   :0.09004  \n                    3rd Qu.:1.202   3rd Qu.:4.869   3rd Qu.:0.09512  \n                    Max.   :1.300   Max.   :6.061   Max.   :0.10656  \n                                                                     \n    accuracy         past_accuracy           ACTUAL       \n Min.   :-0.822085   Min.   :-0.798219   Min.   :-0.0050  \n 1st Qu.:-0.019087   1st Qu.:-0.028736   1st Qu.: 0.0914  \n Median :-0.015035   Median :-0.013423   Median : 0.2643  \n Mean   :-0.048310   Mean   :-0.060652   Mean   : 0.9248  \n 3rd Qu.:-0.005415   3rd Qu.:-0.009260   3rd Qu.: 0.5678  \n Max.   : 0.121449   Max.   :-0.001547   Max.   : 6.0800  \n                     NA's   :1                            \n\n\nCode\n# Create a correlation matrix\ncorrelation_matrix &lt;- cor(NFLX4[, c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\")], use = \"complete.obs\")\n\n\n# Custom color palette\ncolor_palette &lt;- colorRampPalette(c(\"#FFFFFF\", \"#67a9cf\", \"#ef8a62\", \"#b2182b\"))(100)\n\n# Create a correlation plot with enhanced customization\ncorrplot(\n  correlation_matrix,\n  method = \"color\",\n  col = color_palette,\n  type = \"upper\",\n  order = \"original\",\n  tl.cex = 0.7,  # Adjust the size of text labels\n  cl.cex = 0.8,  # Adjust the size of correlation coefficients\n  diag = FALSE,\n  number.cex = 0.8\n)\n\n\n\n\n\nCode\n# Print correlation matrix\nprint(correlation_matrix)\n\n\n                     size  experience    horizon    accuracy past_accuracy\nsize           1.00000000  0.07451284 -0.1317823 -0.04537307    -0.1810330\nexperience     0.07451284  1.00000000 -0.4844637 -0.25882136    -0.4620906\nhorizon       -0.13178225 -0.48446371  1.0000000  0.22264895     0.4979377\naccuracy      -0.04537307 -0.25882136  0.2226489  1.00000000    -0.1604379\npast_accuracy -0.18103301 -0.46209061  0.4979377 -0.16043792     1.0000000\nACTUAL         0.18223220  0.68707354 -0.6346966 -0.31928984    -0.7958850\n                  ACTUAL\nsize           0.1822322\nexperience     0.6870735\nhorizon       -0.6346966\naccuracy      -0.3192898\npast_accuracy -0.7958850\nACTUAL         1.0000000\n\n\nCode\nggpairs(NFLX4, columns = c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\"))\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoving 1 row that contained a missing value\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = size)) +\n  geom_histogram(fill = \"#009999\", color = \"black\", bins = 20) +\n  ggtitle(\"Distribution of Size\") +\n  xlab(\"Size\") +\n  ylab(\"Frequency\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(NFLX4, aes(y = size)) +\n  geom_boxplot(fill = \"#009999\") +\n  ggtitle(\"Boxplot of Size\") +\n  ylab(\"Size\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/post with plot/index.html#introduction",
    "href": "posts/post with plot/index.html#introduction",
    "title": "Understanding Linear Regression Analysis",
    "section": "",
    "text": "Linear Regression Analysis is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The primary goal is to establish a linear equation that best describes the association between these variables. This analysis is widely employed in various fields, including economics, finance, biology, and social sciences, to make predictions and understand the underlying patterns in the data."
  },
  {
    "objectID": "posts/post with plot/index.html#key-components",
    "href": "posts/post with plot/index.html#key-components",
    "title": "Understanding Linear Regression Analysis",
    "section": "Key Components:",
    "text": "Key Components:\nAt its core, linear regression involves fitting a straight line to a set of data points. The equation for a simple linear regression model can be expressed as Y = β₀ + β₁X + ε, where Y is the dependent variable, X is the independent variable, β₀ is the y-intercept, β₁ is the slope of the line, and ε represents the error term. The coefficients β₀ and β₁ are estimated during the analysis, aiming to minimize the sum of squared differences between the observed and predicted values."
  },
  {
    "objectID": "posts/post with plot/index.html#assumptions",
    "href": "posts/post with plot/index.html#assumptions",
    "title": "Understanding Linear Regression Analysis",
    "section": "Assumptions:",
    "text": "Assumptions:\nSeveral assumptions underlie the linear regression model. These include linearity, independence, homoscedasticity, and normality of residuals. Linearity assumes that the relationship between the variables is best represented by a straight line. Independence assumes that the residuals (the differences between observed and predicted values) are not correlated. Homoscedasticity implies that the variability of the residuals remains constant across all levels of the independent variable. Normality of residuals assumes that the residuals follow a normal distribution."
  },
  {
    "objectID": "posts/post with plot/index.html#methodology",
    "href": "posts/post with plot/index.html#methodology",
    "title": "Understanding Linear Regression Analysis",
    "section": "Methodology:",
    "text": "Methodology:\nTo conduct a linear regression analysis, one must collect a dataset comprising paired observations of the dependent and independent variables. The analysis begins by estimating the regression coefficients using methods like the least squares approach. This involves finding the values of β₀ and β₁ that minimize the sum of squared residuals. The quality of the model is often assessed through metrics like the R-squared value, which indicates the proportion of variance in the dependent variable explained by the model."
  },
  {
    "objectID": "posts/post with plot/index.html#applications",
    "href": "posts/post with plot/index.html#applications",
    "title": "Understanding Linear Regression Analysis",
    "section": "Applications:",
    "text": "Applications:\nLinear regression finds applications in diverse fields. In economics, it helps predict economic trends based on various factors. In medicine, it can be used to analyze the relationship between a drug dosage and its therapeutic effect. In marketing, linear regression aids in understanding the impact of advertising expenditures on sales. Its versatility and simplicity make it a valuable tool for extracting meaningful insights from data."
  },
  {
    "objectID": "posts/post with plot/index.html#conclusion",
    "href": "posts/post with plot/index.html#conclusion",
    "title": "Understanding Linear Regression Analysis",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn conclusion, linear regression analysis is a powerful statistical technique that facilitates the exploration and quantification of relationships between variables. Its widespread use across different domains attests to its applicability and reliability in making predictions and understanding patterns within data. By adhering to its underlying assumptions and employing appropriate methodologies, researchers and analysts can harness the full potential of linear regression for informed decision-making."
  },
  {
    "objectID": "posts/post with code/index.html#introduction",
    "href": "posts/post with code/index.html#introduction",
    "title": "Introduction to Linear Regression using SAS Software",
    "section": "",
    "text": "Linear regression is a statistical method widely employed for modeling the relationship between a dependent variable and one or more independent variables. SAS (Statistical Analysis System) software offers a powerful platform for performing linear regression analysis, providing users with robust tools to explore, analyze, and interpret relationships within their datasets.\nSAS’s linear regression capabilities are accessible through procedures such as PROC REG, which stands for regression analysis. This procedure allows users to fit linear models to their data, enabling the estimation of parameters, assessment of model fit, and examination of variable relationships.\nTo initiate a linear regression analysis in SAS, users typically start by specifying the PROC REG statement, followed by the variable names corresponding to the dependent and independent variables. For example: PROC REG DATA=mydata; MODEL dependent_variable = independent_variable1 independent_variable2; RUN; The output from PROC REG includes various tables that provide valuable information for interpreting the linear regression model. The “Model Fit” table, for instance, presents crucial statistics such as the R-squared, which measures the proportion of variability in the dependent variable explained by the model. A higher R-squared indicates a better fit.\nCoefficients for each independent variable are reported in the “Parameter Estimates” table, showing the estimated slope (beta) and intercept (alpha) values. These coefficients help quantify the strength and direction of the relationships between variables.\nDiagnostic statistics, like standard errors, t-values, and p-values, aid in assessing the significance of the coefficients. A low p-value indicates that the corresponding variable significantly contributes to the model. SAS software also facilitates the visualization of linear regression results through graphical tools. The “Plot” statement within PROC REG generates scatterplots with regression lines, aiding in the visual interpretation of the relationships between variables.\nFurthermore, SAS allows users to perform various regression diagnostics, such as checking for multicollinearity, influential observations, and normality of residuals. These diagnostics help ensure the validity of the linear regression model and identify potential issues that may affect its reliability."
  },
  {
    "objectID": "posts/post with code/index.html#conclusion",
    "href": "posts/post with code/index.html#conclusion",
    "title": "Introduction to Linear Regression using SAS Software",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nIn conclusion, SAS software provides a comprehensive and user-friendly environment for conducting linear regression analysis. Through PROC REG and associated tools, users can efficiently build, evaluate, and interpret linear regression models, making informed decisions based on the relationships within their data."
  }
]